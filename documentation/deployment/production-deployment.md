# ğŸš€ Ocean Shopping Center - ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æŒ‡å—

## ğŸ“‹ ç›®éŒ„

- [éƒ¨ç½²æ¦‚è¦½](#éƒ¨ç½²æ¦‚è¦½)
- [ç”Ÿç”¢ç’°å¢ƒæ¶æ§‹](#ç”Ÿç”¢ç’°å¢ƒæ¶æ§‹)  
- [å®¹å™¨åŒ–éƒ¨ç½²](#å®¹å™¨åŒ–éƒ¨ç½²)
- [æ•¸æ“šåº«éƒ¨ç½²](#æ•¸æ“šåº«éƒ¨ç½²)
- [è² è¼‰å‡è¡¡é…ç½®](#è² è¼‰å‡è¡¡é…ç½®)
- [å®‰å…¨é…ç½®](#å®‰å…¨é…ç½®)
- [ç›£æ§å‘Šè­¦](#ç›£æ§å‘Šè­¦)
- [éƒ¨ç½²æµç¨‹](#éƒ¨ç½²æµç¨‹)
- [ç¶­è­·æ“ä½œ](#ç¶­è­·æ“ä½œ)

---

## ğŸ—ï¸ éƒ¨ç½²æ¦‚è¦½

Ocean Shopping Center ç”Ÿç”¢ç’°å¢ƒæ¡ç”¨**å®¹å™¨åŒ–å¾®æœå‹™æ¶æ§‹**ï¼Œæ”¯æŒé«˜å¯ç”¨æ€§ã€è‡ªå‹•æ“´å±•å’Œè—ç¶ éƒ¨ç½²ã€‚ç”Ÿç”¢ç’°å¢ƒåŒ…å«å¤šå€‹å¯¦ä¾‹ä»¥ç¢ºä¿æœå‹™å¯ç”¨æ€§å’Œå®¹éŒ¯èƒ½åŠ›ã€‚

### ç”Ÿç”¢ç’°å¢ƒç‰¹æ€§
- **é«˜å¯ç”¨æ€§**: å¤šå¯¦ä¾‹éƒ¨ç½² + è² è¼‰å‡è¡¡
- **è‡ªå‹•æ“´å±•**: åŸºæ–¼ CPU/Memory æŒ‡æ¨™çš„æ°´å¹³æ“´å±•
- **é›¶åœæ©Ÿéƒ¨ç½²**: è—ç¶ éƒ¨ç½²ç­–ç•¥
- **ç½é›£æ¢å¾©**: æ•¸æ“šå‚™ä»½ + å¿«é€Ÿæ¢å¾©æ©Ÿåˆ¶
- **å®‰å…¨åŠ å›º**: SSL/TLS + é˜²ç«ç‰† + å…¥ä¾µæª¢æ¸¬

---

## ğŸ—ï¸ ç”Ÿç”¢ç’°å¢ƒæ¶æ§‹

### æ•´é«”æ¶æ§‹åœ–

```mermaid
graph TB
    subgraph "Internet & CDN"
        A[Internet Users] --> B[Cloudflare CDN<br/>DDoS Protection]
    end
    
    subgraph "Load Balancer Layer"
        B --> C[Nginx Load Balancer<br/>Primary]
        B --> D[Nginx Load Balancer<br/>Secondary]
    end
    
    subgraph "Application Layer - Zone A"
        C --> E[Frontend Instance 1<br/>React + Nginx]
        C --> F[Backend Instance 1<br/>Spring Boot]
        D --> G[Frontend Instance 2<br/>React + Nginx] 
        D --> H[Backend Instance 2<br/>Spring Boot]
    end
    
    subgraph "Application Layer - Zone B"
        C --> I[Frontend Instance 3<br/>React + Nginx]
        C --> J[Backend Instance 3<br/>Spring Boot]
        D --> K[Frontend Instance 4<br/>React + Nginx]
        D --> L[Backend Instance 4<br/>Spring Boot]
    end
    
    subgraph "Database Layer"
        F --> M[(PostgreSQL Master<br/>Primary DB)]
        H --> M
        J --> M
        L --> M
        M --> N[(PostgreSQL Replica 1<br/>Read Only)]
        M --> O[(PostgreSQL Replica 2<br/>Read Only)]
    end
    
    subgraph "Cache Layer"
        F --> P[(Redis Master<br/>Primary Cache)]
        H --> P
        J --> P
        L --> P
        P --> Q[(Redis Replica 1)]
        P --> R[(Redis Replica 2)]
    end
    
    subgraph "Monitoring & Logging"
        S[Prometheus] --> T[Grafana]
        U[ELK Stack] --> T
        V[Jaeger APM] --> T
        W[AlertManager] --> X[PagerDuty/Slack]
    end
    
    F --> S
    H --> S
    J --> S
    L --> S
    M --> S
    P --> S
```

### éƒ¨ç½²è¦æ¨¡é…ç½®

| çµ„ä»¶ | å¯¦ä¾‹æ•¸é‡ | CPU | Memory | å­˜å„² | å‚™è¨» |
|------|---------|-----|--------|------|------|
| **Frontend** | 4 | 2 Core | 4GB | 50GB | Nginx + React éœæ…‹æ–‡ä»¶ |
| **Backend** | 4 | 4 Core | 8GB | 100GB | Spring Boot æ‡‰ç”¨ |
| **PostgreSQL Master** | 1 | 8 Core | 32GB | 1TB SSD | ä¸»æ•¸æ“šåº« |
| **PostgreSQL Replica** | 2 | 4 Core | 16GB | 1TB SSD | è®€å–å‰¯æœ¬ |
| **Redis Master** | 1 | 4 Core | 16GB | 200GB SSD | ä¸»å¿«å– |
| **Redis Replica** | 2 | 2 Core | 8GB | 200GB SSD | å¿«å–å‰¯æœ¬ |
| **Load Balancer** | 2 | 2 Core | 4GB | 50GB | Nginx è² è¼‰å‡è¡¡ |

---

## ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²

### ç”Ÿç”¢ç’°å¢ƒ Docker Compose

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  # å‰ç«¯æ‡‰ç”¨
  frontend:
    image: ocean-shopping-center/frontend:${VERSION}
    deploy:
      replicas: 4
      restart_policy:
        condition: on-failure
        max_attempts: 3
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    networks:
      - frontend-network
    environment:
      - NODE_ENV=production
      - REACT_APP_API_URL=${API_URL}
      - REACT_APP_WEBSOCKET_URL=${WEBSOCKET_URL}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # å¾Œç«¯æ‡‰ç”¨
  backend:
    image: ocean-shopping-center/backend:${VERSION}
    deploy:
      replicas: 4
      restart_policy:
        condition: on-failure
        max_attempts: 3
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
    networks:
      - backend-network
      - database-network
    environment:
      - SPRING_PROFILES_ACTIVE=production
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - JWT_SECRET=${JWT_SECRET}
      - STRIPE_SECRET_KEY=${STRIPE_SECRET_KEY}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    depends_on:
      - postgres-master
      - redis-master

  # PostgreSQL ä¸»æ•¸æ“šåº«
  postgres-master:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_REPLICATION_USER=${POSTGRES_REPLICATION_USER}
      - POSTGRES_REPLICATION_PASSWORD=${POSTGRES_REPLICATION_PASSWORD}
    volumes:
      - postgres-master-data:/var/lib/postgresql/data
      - ./postgres/postgresql.conf:/etc/postgresql/postgresql.conf
      - ./postgres/pg_hba.conf:/etc/postgresql/pg_hba.conf
    networks:
      - database-network
    ports:
      - "5432:5432"
    command: >
      postgres
      -c config_file=/etc/postgresql/postgresql.conf
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # PostgreSQL è®€å–å‰¯æœ¬ 1
  postgres-replica1:
    image: postgres:15-alpine
    environment:
      - PGUSER=${POSTGRES_REPLICATION_USER}
      - POSTGRES_PASSWORD=${POSTGRES_REPLICATION_PASSWORD}
      - POSTGRES_MASTER_SERVICE=postgres-master
    volumes:
      - postgres-replica1-data:/var/lib/postgresql/data
    networks:
      - database-network
    depends_on:
      - postgres-master
    command: |
      bash -c '
      until pg_basebackup --pgdata=/var/lib/postgresql/data -R --slot=replication_slot_1 --host=postgres-master --port=5432
      do
        echo "Waiting for master to connect..."
        sleep 1s
      done
      echo "Backup done, starting replica..."
      chmod 0700 /var/lib/postgresql/data
      postgres
      '

  # Redis ä¸»å¿«å–
  redis-master:
    image: redis:7-alpine
    networks:
      - database-network
    volumes:
      - redis-master-data:/data
      - ./redis/redis.conf:/etc/redis/redis.conf
    command: redis-server /etc/redis/redis.conf
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis å‰¯æœ¬
  redis-replica1:
    image: redis:7-alpine
    networks:
      - database-network
    volumes:
      - redis-replica1-data:/data
    command: redis-server --slaveof redis-master 6379
    depends_on:
      - redis-master

  # Nginx è² è¼‰å‡è¡¡å™¨
  nginx-lb:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./nginx/logs:/var/log/nginx
    networks:
      - frontend-network
      - backend-network
    depends_on:
      - frontend
      - backend
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus ç›£æ§
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    networks:
      - monitoring-network

  # Grafana å„€è¡¨æ¿
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    networks:
      - monitoring-network
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}

networks:
  frontend-network:
    driver: bridge
  backend-network:
    driver: bridge
  database-network:
    driver: bridge
    internal: true
  monitoring-network:
    driver: bridge

volumes:
  postgres-master-data:
  postgres-replica1-data:
  postgres-replica2-data:
  redis-master-data:
  redis-replica1-data:
  redis-replica2-data:
  prometheus-data:
  grafana-data:
```

### ç”Ÿç”¢ç’°å¢ƒé…ç½®æª”æ¡ˆ

#### Nginx è² è¼‰å‡è¡¡é…ç½®

```nginx
# nginx/nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream frontend {
        least_conn;
        server frontend:3000 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }
    
    upstream backend {
        least_conn;
        server backend:8080 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }
    
    # é™é€Ÿé…ç½®
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=login:10m rate=5r/m;
    
    server {
        listen 80;
        server_name your-domain.com;
        
        # HTTPS é‡å®šå‘
        return 301 https://$server_name$request_uri;
    }
    
    server {
        listen 443 ssl http2;
        server_name your-domain.com;
        
        # SSL é…ç½®
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256;
        
        # å®‰å…¨æ¨™é ­
        add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
        add_header X-Frame-Options DENY always;
        add_header X-Content-Type-Options nosniff always;
        add_header X-XSS-Protection "1; mode=block" always;
        
        # å‰ç«¯éœæ…‹è³‡æº
        location / {
            proxy_pass http://frontend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # å¿«å–é…ç½®
            expires 1d;
            add_header Cache-Control "public, immutable";
        }
        
        # API è·¯ç”±
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # è¶…æ™‚é…ç½®
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }
        
        # ç™»å…¥ API ç‰¹æ®Šé™é€Ÿ
        location /api/auth/login {
            limit_req zone=login burst=5 nodelay;
            
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
        
        # WebSocket æ”¯æŒ
        location /ws/ {
            proxy_pass http://backend;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
        
        # å¥åº·æª¢æŸ¥
        location /health {
            access_log off;
            return 200 "OK\n";
            add_header Content-Type text/plain;
        }
    }
}
```

---

## ğŸ—„ï¸ æ•¸æ“šåº«éƒ¨ç½²

### PostgreSQL ä¸»å¾é…ç½®

#### ä¸»æ•¸æ“šåº«é…ç½® (postgresql.conf)

```ini
# postgresql.conf
listen_addresses = '*'
port = 5432
max_connections = 200
shared_buffers = 8GB
effective_cache_size = 24GB
maintenance_work_mem = 2GB
checkpoint_completion_target = 0.9
wal_buffers = 16MB
default_statistics_target = 100
random_page_cost = 1.1
effective_io_concurrency = 200

# è¤‡è£½é…ç½®
wal_level = replica
archive_mode = on
archive_command = 'cp %p /var/lib/postgresql/archive/%f'
max_wal_senders = 3
max_replication_slots = 3
hot_standby = on
hot_standby_feedback = on

# æ—¥èªŒé…ç½®
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_statement = 'mod'
log_min_duration_statement = 1000
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on
```

#### è¤‡è£½ç”¨æˆ¶é…ç½® (pg_hba.conf)

```conf
# pg_hba.conf
local   all             postgres                                peer
local   all             all                                     peer
host    all             all             0.0.0.0/0              md5
host    replication     replicator      0.0.0.0/0              md5
```

### Redis é›†ç¾¤é…ç½®

```conf
# redis/redis.conf
bind 0.0.0.0
port 6379
protected-mode yes
requirepass your_redis_password

# æŒä¹…åŒ–é…ç½®
save 900 1
save 300 10
save 60 10000

# è¨˜æ†¶é«”é…ç½®
maxmemory 12gb
maxmemory-policy allkeys-lru

# æ—¥èªŒé…ç½®
loglevel notice
logfile /var/log/redis/redis-server.log

# å®‰å…¨é…ç½®
rename-command FLUSHALL ""
rename-command FLUSHDB ""
rename-command KEYS ""
```

---

## âš–ï¸ è² è¼‰å‡è¡¡é…ç½®

### è² è¼‰å‡è¡¡ç­–ç•¥

```mermaid
graph TB
    subgraph "Load Balancing Strategy"
        A[Nginx Load Balancer] --> B[Health Check<br/>Every 30s]
        A --> C[Least Connections<br/>Algorithm]
        A --> D[Failover Detection<br/>3 fails = offline]
    end
    
    subgraph "Backend Instances"
        E[Backend 1<br/>Healthy]
        F[Backend 2<br/>Healthy]
        G[Backend 3<br/>Failed]
        H[Backend 4<br/>Healthy]
    end
    
    C --> E
    C --> F
    C --> H
    D --> G
```

### æœƒè©±ä¿æŒé…ç½®

```nginx
# æœƒè©±ä¿æŒ (å¦‚éœ€è¦)
upstream backend {
    ip_hash;  # åŸºæ–¼ IP çš„æœƒè©±ä¿æŒ
    server backend1:8080;
    server backend2:8080;
    server backend3:8080;
    server backend4:8080;
}
```

---

## ğŸ”’ å®‰å…¨é…ç½®

### SSL/TLS é…ç½®

#### ç”³è«‹å’Œé…ç½® SSL è­‰æ›¸

```bash
# ä½¿ç”¨ Let's Encrypt ç”³è«‹å…è²» SSL
sudo certbot --nginx -d your-domain.com -d www.your-domain.com

# è¨­ç½®è‡ªå‹•çºŒç´„
sudo crontab -e
# æ·»åŠ ä»¥ä¸‹è¡Œ
0 12 * * * /usr/bin/certbot renew --quiet
```

### é˜²ç«ç‰†é…ç½®

```bash
# UFW é˜²ç«ç‰†è¦å‰‡
sudo ufw default deny incoming
sudo ufw default allow outgoing

# å…è¨±å¿…è¦ç«¯å£
sudo ufw allow ssh
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp

# é™åˆ¶æ•¸æ“šåº«è¨ªå•
sudo ufw allow from 10.0.0.0/8 to any port 5432
sudo ufw allow from 10.0.0.0/8 to any port 6379

# å•Ÿç”¨é˜²ç«ç‰†
sudo ufw enable
```

### æ‡‰ç”¨ç¨‹å¼å®‰å…¨é…ç½®

```yaml
# application-production.yml
spring:
  security:
    cors:
      allowed-origins:
        - "https://your-domain.com"
        - "https://www.your-domain.com"
      allowed-methods:
        - GET
        - POST
        - PUT
        - DELETE
        - OPTIONS
      allowed-headers: "*"
      allow-credentials: true
      max-age: 3600

  datasource:
    hikari:
      maximum-pool-size: 20
      minimum-idle: 5
      connection-timeout: 30000
      idle-timeout: 600000
      max-lifetime: 1800000

jwt:
  secret: ${JWT_SECRET}
  expiration: 3600000  # 1 hour
  refresh-expiration: 2592000000  # 30 days

logging:
  level:
    org.springframework.security: INFO
    org.hibernate.SQL: WARN
    org.hibernate.type.descriptor.sql.BasicBinder: WARN
```

---

## ğŸ“Š ç›£æ§å‘Šè­¦

### Prometheus é…ç½®

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'spring-boot-app'
    static_configs:
      - targets: ['backend:8080']
    metrics_path: '/actuator/prometheus'
    scrape_interval: 10s

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    scrape_interval: 10s

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    scrape_interval: 10s

  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx-exporter:9113']
    scrape_interval: 10s

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

### å‘Šè­¦è¦å‰‡é…ç½®

```yaml
# monitoring/alert_rules.yml
groups:
  - name: application_alerts
    rules:
      - alert: HighCPUUsage
        expr: cpu_usage_percent > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 90% for more than 5 minutes"

      - alert: HighMemoryUsage
        expr: memory_usage_percent > 95
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is above 95% for more than 5 minutes"

      - alert: DatabaseConnectionPoolHigh
        expr: hikari_connections_active / hikari_connections_max > 0.9
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Database connection pool usage high"
          description: "Connection pool usage is above 90%"

      - alert: ApplicationDown
        expr: up{job="spring-boot-app"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Application instance down"
          description: "Application instance has been down for more than 1 minute"

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.01
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "HTTP 5xx error rate is above 1% for more than 5 minutes"
```

---

## ğŸš€ éƒ¨ç½²æµç¨‹

### è—ç¶ éƒ¨ç½²ç­–ç•¥

```mermaid
sequenceDiagram
    participant D as Developer
    participant CI as CI/CD Pipeline
    participant LB as Load Balancer
    participant G as Green Environment<br/>(Current)
    participant B as Blue Environment<br/>(New)
    participant DB as Database
    participant M as Monitoring

    D->>CI: Git Push (æ–°ç‰ˆæœ¬)
    CI->>CI: å»ºæ§‹æ˜ åƒæª”
    CI->>B: éƒ¨ç½²åˆ° Blue ç’°å¢ƒ
    B->>DB: é‹è¡Œè³‡æ–™åº«é·ç§»
    CI->>B: é‹è¡Œå¥åº·æª¢æŸ¥
    B->>CI: å¥åº·æª¢æŸ¥é€šé
    
    CI->>LB: åˆ‡æ›æµé‡åˆ° Blue
    LB->>B: è·¯ç”±æ–°è«‹æ±‚
    CI->>M: ç›£æ§æ–°ç’°å¢ƒ
    
    Note over CI,M: è§€å¯ŸæœŸ (15åˆ†é˜)
    
    alt éƒ¨ç½²æˆåŠŸ
        CI->>G: åœæ­¢ Green ç’°å¢ƒ
        CI->>D: éƒ¨ç½²æˆåŠŸé€šçŸ¥
    else éƒ¨ç½²å¤±æ•—
        CI->>LB: å›æ»¾åˆ° Green
        LB->>G: æ¢å¾©æµé‡
        CI->>D: éƒ¨ç½²å¤±æ•—é€šçŸ¥
    end
```

### éƒ¨ç½²è…³æœ¬

```bash
#!/bin/bash
# deploy.sh

set -e  # é‡åˆ°éŒ¯èª¤ç«‹å³é€€å‡º

# ç’°å¢ƒè®Šæ•¸
ENVIRONMENT=${1:-production}
VERSION=${2:-latest}
BLUE_GREEN=${3:-blue}

echo "ğŸš€ é–‹å§‹éƒ¨ç½² Ocean Shopping Center"
echo "ç’°å¢ƒ: $ENVIRONMENT"
echo "ç‰ˆæœ¬: $VERSION" 
echo "ç’°å¢ƒ: $BLUE_GREEN"

# 1. æª¢æŸ¥å…ˆæ±ºæ¢ä»¶
echo "âœ… æª¢æŸ¥éƒ¨ç½²ç’°å¢ƒ..."
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker æœªå®‰è£"
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "âŒ Docker Compose æœªå®‰è£"
    exit 1
fi

# 2. è¼‰å…¥ç’°å¢ƒé…ç½®
echo "ğŸ“‹ è¼‰å…¥ç’°å¢ƒé…ç½®..."
source .env.$ENVIRONMENT

# 3. å»ºæ§‹æ‡‰ç”¨æ˜ åƒæª”
echo "ğŸ—ï¸ å»ºæ§‹æ‡‰ç”¨æ˜ åƒæª”..."
docker build -t ocean-shopping-center/frontend:$VERSION ./frontend
docker build -t ocean-shopping-center/backend:$VERSION ./backend

# 4. è³‡æ–™åº«å‚™ä»½
echo "ğŸ’¾ å‚™ä»½è³‡æ–™åº«..."
docker-compose -f docker-compose.$ENVIRONMENT.yml exec -T postgres-master \
    pg_dump -U $POSTGRES_USER $POSTGRES_DB > backup-$(date +%Y%m%d-%H%M%S).sql

# 5. éƒ¨ç½²åˆ°è—ç¶ ç’°å¢ƒ
echo "ğŸ”„ éƒ¨ç½²åˆ° $BLUE_GREEN ç’°å¢ƒ..."
export VERSION=$VERSION
export ENVIRONMENT=$BLUE_GREEN

docker-compose -f docker-compose.$ENVIRONMENT.yml up -d

# 6. å¥åº·æª¢æŸ¥
echo "ğŸ¥ ç­‰å¾…æœå‹™å¥åº·æª¢æŸ¥..."
sleep 60

# æª¢æŸ¥å‰ç«¯å¥åº·
for i in {1..30}; do
    if curl -f http://localhost:3000/health > /dev/null 2>&1; then
        echo "âœ… å‰ç«¯æœå‹™å¥åº·"
        break
    fi
    if [ $i -eq 30 ]; then
        echo "âŒ å‰ç«¯å¥åº·æª¢æŸ¥å¤±æ•—"
        exit 1
    fi
    sleep 10
done

# æª¢æŸ¥å¾Œç«¯å¥åº·
for i in {1..30}; do
    if curl -f http://localhost:8080/actuator/health > /dev/null 2>&1; then
        echo "âœ… å¾Œç«¯æœå‹™å¥åº·"
        break
    fi
    if [ $i -eq 30 ]; then
        echo "âŒ å¾Œç«¯å¥åº·æª¢æŸ¥å¤±æ•—"
        exit 1
    fi
    sleep 10
done

# 7. æµé‡åˆ‡æ› (éœ€è¦æ‰‹å‹•ç¢ºèª)
echo "âš ï¸ æº–å‚™åˆ‡æ›æµé‡åˆ°æ–°ç’°å¢ƒ"
echo "è«‹ç¢ºèªæ–°ç’°å¢ƒé‹è¡Œæ­£å¸¸ï¼Œç„¶å¾ŒåŸ·è¡Œ:"
echo "  ./switch-traffic.sh $BLUE_GREEN"

# 8. æ¸…ç†èˆŠæ˜ åƒæª”
echo "ğŸ§¹ æ¸…ç†èˆŠæ˜ åƒæª”..."
docker image prune -a -f --filter "until=24h"

echo "âœ… éƒ¨ç½²å®Œæˆï¼"
echo "ğŸ“Š ç›£æ§é¢æ¿: http://your-domain.com:3001"
echo "ğŸ“Š API æ–‡æª”: https://your-domain.com/api/swagger-ui.html"
```

### æµé‡åˆ‡æ›è…³æœ¬

```bash
#!/bin/bash
# switch-traffic.sh

ENVIRONMENT=${1:-blue}

echo "ğŸ”„ åˆ‡æ›æµé‡åˆ° $ENVIRONMENT ç’°å¢ƒ..."

# æ›´æ–° Nginx é…ç½®
if [ "$ENVIRONMENT" = "blue" ]; then
    cp nginx/nginx-blue.conf nginx/nginx.conf
else
    cp nginx/nginx-green.conf nginx/nginx.conf
fi

# é‡æ–°è¼‰å…¥ Nginx é…ç½®
docker-compose -f docker-compose.prod.yml exec nginx-lb nginx -s reload

echo "âœ… æµé‡å·²åˆ‡æ›åˆ° $ENVIRONMENT ç’°å¢ƒ"

# ç›£æ§æ–°ç’°å¢ƒ 15 åˆ†é˜
echo "ğŸ“Š ç›£æ§æ–°ç’°å¢ƒ 15 åˆ†é˜..."
for i in {1..15}; do
    echo "ç›£æ§ä¸­... ($i/15 åˆ†é˜)"
    
    # æª¢æŸ¥éŒ¯èª¤ç‡
    ERROR_RATE=$(curl -s 'http://prometheus:9090/api/v1/query?query=rate(http_requests_total{status=~"5.."}[5m])' | jq '.data.result[0].value[1]' | sed 's/"//g')
    
    if (( $(echo "$ERROR_RATE > 0.01" | bc -l) )); then
        echo "âŒ éŒ¯èª¤ç‡éé«˜ï¼Œæº–å‚™å›æ»¾"
        ./rollback.sh
        exit 1
    fi
    
    sleep 60
done

echo "âœ… éƒ¨ç½²æˆåŠŸï¼Œæ–°ç’°å¢ƒé‹è¡Œç©©å®š"
```

### å›æ»¾è…³æœ¬

```bash
#!/bin/bash
# rollback.sh

echo "ğŸ”„ é–‹å§‹å›æ»¾éƒ¨ç½²..."

# ç¢ºå®šç•¶å‰ç’°å¢ƒ
CURRENT_ENV=$(docker-compose -f docker-compose.prod.yml ps | grep "Up" | grep -q "blue" && echo "blue" || echo "green")
ROLLBACK_ENV=$([ "$CURRENT_ENV" = "blue" ] && echo "green" || echo "blue")

echo "ç•¶å‰ç’°å¢ƒ: $CURRENT_ENV"
echo "å›æ»¾åˆ°ç’°å¢ƒ: $ROLLBACK_ENV"

# åˆ‡æ›æµé‡
./switch-traffic.sh $ROLLBACK_ENV

# åœæ­¢æœ‰å•é¡Œçš„ç’°å¢ƒ
echo "ğŸ›‘ åœæ­¢æœ‰å•é¡Œçš„ç’°å¢ƒ..."
docker-compose -f docker-compose.$CURRENT_ENV.yml down

echo "âœ… å›æ»¾å®Œæˆ"
```

---

## ğŸ”§ ç¶­è­·æ“ä½œ

### æ—¥å¸¸ç¶­è­·æª¢æŸ¥æ¸…å–®

#### æ¯æ—¥æª¢æŸ¥
- [ ] æª¢æŸ¥æ‡‰ç”¨ç¨‹å¼å¥åº·ç‹€æ…‹
- [ ] æª¢æŸ¥ç³»çµ±è³‡æºä½¿ç”¨ç‡ (CPU, Memory, Disk)
- [ ] æª¢æŸ¥æ•¸æ“šåº«é€£æ¥æ± ç‹€æ…‹
- [ ] æª¢æŸ¥å¿«å–å‘½ä¸­ç‡
- [ ] æª¢æŸ¥éŒ¯èª¤æ—¥èªŒ

#### æ¯é€±æª¢æŸ¥  
- [ ] æ•¸æ“šåº«æ•ˆèƒ½åˆ†æ
- [ ] å®‰å…¨æ—¥èªŒå¯©æŸ¥
- [ ] å‚™ä»½æ¢å¾©æ¸¬è©¦
- [ ] ä¾è³´é …å®‰å…¨æƒæ
- [ ] ç›£æ§å‘Šè­¦è¦å‰‡æª¢æŸ¥

#### æ¯æœˆæª¢æŸ¥
- [ ] ç³»çµ±å®‰å…¨æ›´æ–°
- [ ] å®¹é‡è¦åŠƒè©•ä¼°
- [ ] ç½é›£æ¢å¾©æ¼”ç·´
- [ ] æ•ˆèƒ½åŸºæº–æ¸¬è©¦
- [ ] æ–‡æª”æ›´æ–°

### å‚™ä»½èˆ‡æ¢å¾©

#### æ•¸æ“šåº«å‚™ä»½è…³æœ¬

```bash
#!/bin/bash
# backup-database.sh

BACKUP_DIR="/var/backups/postgresql"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/ocean-shopping-center-$DATE.sql"

# å‰µå»ºå‚™ä»½ç›®éŒ„
mkdir -p $BACKUP_DIR

# åŸ·è¡Œå‚™ä»½
docker-compose -f docker-compose.prod.yml exec -T postgres-master \
    pg_dump -U $POSTGRES_USER $POSTGRES_DB > $BACKUP_FILE

# å£“ç¸®å‚™ä»½
gzip $BACKUP_FILE

# æ¸…ç†èˆŠå‚™ä»½ (ä¿ç•™ 30 å¤©)
find $BACKUP_DIR -name "*.gz" -mtime +30 -delete

echo "âœ… æ•¸æ“šåº«å‚™ä»½å®Œæˆ: $BACKUP_FILE.gz"
```

#### æ•¸æ“šæ¢å¾©è…³æœ¬

```bash
#!/bin/bash
# restore-database.sh

BACKUP_FILE=${1}

if [ -z "$BACKUP_FILE" ]; then
    echo "âŒ è«‹æŒ‡å®šå‚™ä»½æ–‡ä»¶"
    echo "ç”¨æ³•: ./restore-database.sh /path/to/backup.sql.gz"
    exit 1
fi

# ç¢ºèªæ¢å¾©æ“ä½œ
read -p "âš ï¸ é€™å°‡è¦†è“‹ç•¶å‰æ•¸æ“šåº«ï¼Œç¢ºå®šè¦ç¹¼çºŒå—ï¼Ÿ (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    exit 1
fi

# è§£å£“å‚™ä»½æ–‡ä»¶
echo "ğŸ“¦ è§£å£“å‚™ä»½æ–‡ä»¶..."
if [[ $BACKUP_FILE == *.gz ]]; then
    gunzip -c $BACKUP_FILE > temp_restore.sql
    RESTORE_FILE=temp_restore.sql
else
    RESTORE_FILE=$BACKUP_FILE
fi

# åœæ­¢æ‡‰ç”¨ç¨‹å¼
echo "ğŸ›‘ åœæ­¢æ‡‰ç”¨ç¨‹å¼..."
docker-compose -f docker-compose.prod.yml stop backend

# æ¢å¾©æ•¸æ“šåº«
echo "ğŸ”„ æ¢å¾©æ•¸æ“šåº«..."
docker-compose -f docker-compose.prod.yml exec -T postgres-master \
    psql -U $POSTGRES_USER -d $POSTGRES_DB < $RESTORE_FILE

# æ¸…ç†è‡¨æ™‚æ–‡ä»¶
if [ "$RESTORE_FILE" = "temp_restore.sql" ]; then
    rm temp_restore.sql
fi

# é‡å•Ÿæ‡‰ç”¨ç¨‹å¼
echo "ğŸš€ é‡å•Ÿæ‡‰ç”¨ç¨‹å¼..."
docker-compose -f docker-compose.prod.yml start backend

echo "âœ… æ•¸æ“šåº«æ¢å¾©å®Œæˆ"
```

### æ—¥èªŒç®¡ç†

#### æ—¥èªŒæ”¶é›†é…ç½®

```yaml
# docker-compose.logging.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    networks:
      - logging-network

  logstash:
    image: docker.elastic.co/logstash/logstash:8.8.0
    volumes:
      - ./logging/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - ./logs:/var/log/app
    networks:
      - logging-network
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    ports:
      - "5601:5601"
    networks:
      - logging-network
    depends_on:
      - elasticsearch

networks:
  logging-network:
    driver: bridge

volumes:
  elasticsearch-data:
```

### æ€§èƒ½èª¿å„ª

#### JVM èª¿å„ª

```yaml
# backend service environment variables
environment:
  - JAVA_OPTS=-Xms4g -Xmx6g -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+PrintGCDetails
```

#### æ•¸æ“šåº«èª¿å„ª

```sql
-- å¸¸ç”¨æŸ¥è©¢å„ªåŒ–
EXPLAIN (ANALYZE, BUFFERS) 
SELECT * FROM orders 
WHERE user_id = 123 AND created_at >= '2023-01-01';

-- å»ºç«‹ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_orders_user_id_created_at 
ON orders(user_id, created_at);

-- åˆ†æè¡¨çµ±è¨ˆä¿¡æ¯
ANALYZE orders;
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

| å•é¡Œ | ç—‡ç‹€ | è§£æ±ºæ–¹æ¡ˆ |
|------|------|----------|
| **é«˜ CPU ä½¿ç”¨ç‡** | éŸ¿æ‡‰ç·©æ…¢ | 1. æª¢æŸ¥æ…¢æŸ¥è©¢<br/>2. å¢åŠ å¯¦ä¾‹æ•¸é‡<br/>3. èª¿å„ª JVM åƒæ•¸ |
| **è¨˜æ†¶é«”ä¸è¶³** | OOMKilled | 1. å¢åŠ è¨˜æ†¶é«”é™åˆ¶<br/>2. æª¢æŸ¥è¨˜æ†¶é«”æ´©æ¼<br/>3. èª¿æ•´ JVM heap size |
| **è³‡æ–™åº«é€£æ¥è€—ç›¡** | Connection timeout | 1. å¢åŠ é€£æ¥æ± å¤§å°<br/>2. æª¢æŸ¥é€£æ¥æ´©æ¼<br/>3. æ¸›å°‘é€£æ¥è¶…æ™‚æ™‚é–“ |
| **å¿«å–æœªå‘½ä¸­** | æŸ¥è©¢ç·©æ…¢ | 1. æª¢æŸ¥å¿«å–é…ç½®<br/>2. é ç†±å¿«å–<br/>3. èª¿æ•´ TTL è¨­ç½® |

### ç·Šæ€¥è¯çµ¡è³‡è¨Š

- **ç³»çµ±ç®¡ç†å“¡**: admin@ocean-shopping.com
- **é–‹ç™¼åœ˜éšŠ**: dev-team@ocean-shopping.com
- **æ¥­å‹™è¯çµ¡äºº**: business@ocean-shopping.com
- **24/7 æ”¯æ´**: +886-xxx-xxx-xxx

---

## ğŸ“– ç›¸é—œæ–‡æª”

- [ç³»çµ±æ¶æ§‹è¨­è¨ˆ](../architecture/system-architecture.md)
- [é–‹ç™¼ç’°å¢ƒè¨­ç½®](development-setup.md)
- [API æ–‡æª”](../api/api-documentation.md)
- [ç›£æ§å‘Šè­¦](../monitoring/monitoring-alerting.md)

---

**æœ€å¾Œæ›´æ–°**: 2025-09-05  
**ç‰ˆæœ¬**: 1.0  
**ç¶­è­·è€…**: Ocean Shopping Center Team